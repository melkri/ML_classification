{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"C:/Users/micha/OneDrive/Dokumenty/GitHub/Machine_Learning/ML_classification\")\n",
    "\n",
    "# df -> data with part of the feature selection on whole df and part on train\n",
    "# df_new -> data with feature selection on train df\n",
    "\n",
    "df_old = pd.read_excel('../data/input_processed/train.xlsx', index_col=0)\n",
    "test_old = pd.read_excel('../data/input_processed/test.xlsx', index_col=0)\n",
    "fr_old = pd.read_excel('../data/input_processed/feature_ranking.xlsx', index_col=0)\n",
    "\n",
    "df_new = pd.read_excel('../data/input_processed/train_1.xlsx', index_col=0)\n",
    "test_new = pd.read_excel('../data/input_processed/test_1.xlsx', index_col=0)\n",
    "fr_new = pd.read_excel('../data/input_processed/feature_ranking_1.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(df, x_col, model, param_grid, y_col, cv):\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='balanced_accuracy', cv=cv)\n",
    "\n",
    "    grid_search.fit(df.loc[:, x_col], df.loc[:, y_col])\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(df.loc[:, x_col])\n",
    "\n",
    "    accuracy = balanced_accuracy_score(df.loc[:, y_col], y_pred)\n",
    "    confusion = confusion_matrix(df.loc[:, y_col], y_pred)\n",
    "\n",
    "    print(f\"On Data: {x_col}, and model {model}\")\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Best model has parameters:\")\n",
    "    print(best_model)\n",
    "\n",
    "    return model, accuracy\n",
    "\n",
    "def get_model_name(model):\n",
    "    return model.__class__.__name__.split(\"(\")[0]\n",
    "\n",
    "def cross_validation(model, _X, _y, _cv=5):\n",
    "      _scoring = ['balanced_accuracy']\n",
    "      results = cross_validate(estimator=model,\n",
    "                               X=_X,\n",
    "                               y=_y,\n",
    "                               cv=_cv,\n",
    "                               scoring=_scoring,\n",
    "                               return_train_score=True)\n",
    "      val_acc_scores = results['test_balanced_accuracy']\n",
    "      val_acc_mean = np.mean(val_acc_scores) * 100\n",
    "      val_acc_std = np.std(val_acc_scores) * 100\n",
    "\n",
    "      return val_acc_mean, val_acc_std\n",
    "      #return {\"Training Accuracy scores\": results['train_balanced_accuracy'],\n",
    "       #       \"Mean Training Accuracy\": results['train_balanced_accuracy'].mean()*100,\n",
    "        #      \n",
    "         #     \"Validation Accuracy scores\": results['test_balanced_accuracy'],\n",
    "          #    \"Mean Validation Accuracy\": results['test_balanced_accuracy'].mean()*100,\n",
    "           #   }\n",
    "           \n",
    "def cross_validation_accuracy(models, df, x_cols, y_col, cv=5):\n",
    "    results_df = pd.DataFrame(columns=['Data Type', 'Mean Validation Accuracy', 'Validation Accuracy Std. Dev.'])\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = get_model_name(model)\n",
    "        \n",
    "        for x_col_name, x_col in x_cols.items():\n",
    "            scores = cross_validate(model, df.loc[:, x_col], df.loc[:, y_col], cv=cv, scoring='balanced_accuracy', return_train_score=True)\n",
    "            mean_accuracy = np.mean(scores['test_score']) * 100\n",
    "            std_accuracy = np.std(scores['test_score']) * 100\n",
    "            \n",
    "            results_df = results_df.append({'Data Type': x_col_name, 'Mean Validation Accuracy': mean_accuracy, 'Validation Accuracy Std. Dev.': std_accuracy, 'Model': model_name}, ignore_index=True)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu są definiowane których kolumn będziemy później używać za pomocą kryteriów wyliczonych na koniec 01_Data_preparation\n",
    "# Jeśli masz w głowie jakieś inne to możesz dodać w 01, żeby były uwzględnione w feature_ranking.xlsx, oni tam podawali \n",
    "# FRE np. możesz spojrzeć w feature selection na wykład. \n",
    "\n",
    "boruta = fr_old[fr_old['boruta_rank'].isin([1])].index.tolist()\n",
    "mi_score = fr_old[fr_old['mi_score'] > 0.01].index.tolist()\n",
    "f_score = fr_old[fr_old['sign_fscore_0_1'] == 1].index.tolist()\n",
    "Importance = fr_old[fr_old['Importance'] > 0.01].index.tolist()\n",
    "Correlation = fr_old[fr_old['Corr'] > 0.1].index.tolist()\n",
    "y_col = 'account_status'\n",
    "\n",
    "x_cols_old = {\n",
    "    'boruta': boruta,\n",
    "    'mi_score': mi_score,\n",
    "    'f_score': f_score,\n",
    "    'Importance': Importance,\n",
    "    'Correlation':Correlation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zdefioniowanie x_cols_new\n",
    "\n",
    "boruta = fr_new[fr_new['boruta_rank'].isin([1])].index.tolist()\n",
    "mi_score = fr_new[fr_new['mi_score'] > 0.01].index.tolist()\n",
    "f_score = fr_new[fr_new['sign_fscore_0_1'] == 1].index.tolist()\n",
    "Importance = fr_new[fr_new['Importance'] > 0.01].index.tolist()\n",
    "Correlation = fr_new[fr_new['Corr'] > 0.1].index.tolist()\n",
    "y_col = 'account_status'\n",
    "\n",
    "x_cols_new = {\n",
    "    'boruta': boruta,\n",
    "    'mi_score': mi_score,\n",
    "    'f_score': f_score,\n",
    "    'Importance': Importance,\n",
    "    'Correlation':Correlation\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTAJ TUNUJEMY HYPERPARAMETRY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging zastosowny do:\n",
    "- Decision tree (nazwa BaggingClassifier - nazwać odpowiednio do decision tree)\n",
    "\n",
    "modele które mamy:\n",
    "- SVC\n",
    "- LogisticRegression\n",
    "- Decision tree\n",
    "\n",
    "modele do dorzucenia:\n",
    "- KNN\n",
    "\n",
    "cv po wyestymowaniu zrobić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10,20],\n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "results_df = pd.DataFrame(columns=['Data Type', 'Accuracy'])\n",
    "\n",
    "# Tu definiujesz zbiór parametrów na przestrzeni której będzies szukać najlepszego doboru parametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search SVM for old df\n",
    "\n",
    "for x_col in x_cols_old:\n",
    "    accuracy = grid_search(df_old, x_cols_old[x_col], SVC(), param_grid, y_col, 5)\n",
    "    results_df = results_df.append({'Data Type': x_col, 'Accuracy': accuracy}, ignore_index=True)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to other df\n",
    "results_df_SVM_old = results_df\n",
    "results_df_SVM_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search SVM for new df \n",
    "results_df = pd.DataFrame(columns=['Data Type', 'Accuracy'])\n",
    "\n",
    "for x_col in x_cols_new:\n",
    "    accuracy = grid_search(df_new, x_cols_new[x_col], SVC(), param_grid, y_col, 5)\n",
    "    results_df = results_df.append({'Data Type': x_col, 'Accuracy': accuracy}, ignore_index=True)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to other df\n",
    "results_df_SVM_new = results_df\n",
    "results_df_SVM_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_SVC = pd.concat([results_df_SVM_old, results_df_SVM_new], axis=1)\n",
    "df_compare_SVC.columns = ['Data Type', 'Accuracy Old', 'Data Type New', 'Accuracy New']\n",
    "df_compare_SVC = df_compare_SVC.drop(columns=['Data Type New'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search DT for old df\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "results_df = pd.DataFrame(columns=['Data Type', 'Accuracy'])\n",
    "\n",
    "for x_col in x_cols_old:\n",
    "    accuracy = grid_search(df_old, x_cols_old[x_col], model, param_grid, y_col, 5)\n",
    "    results_df = results_df.append({'Data Type': x_col, 'Accuracy': accuracy}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DT_old = results_df\n",
    "results_DT_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search DT for new df\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Data Type', 'Accuracy'])\n",
    "\n",
    "for x_col in x_cols_new:\n",
    "    accuracy = grid_search(df_new, x_cols_new[x_col], model, param_grid, y_col, 5)\n",
    "    results_df = results_df.append({'Data Type': x_col, 'Accuracy': accuracy}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DT_new = results_df\n",
    "results_DT_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_DT = pd.concat([results_DT_old, results_DT_new], axis=1)\n",
    "df_compare_DT.columns = ['Data Type', 'Accuracy Old', 'Data Type New', 'Accuracy New']\n",
    "df_compare_DT = df_compare_DT.drop(columns=['Data Type New'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_DT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']  # Some solvers only support certain types of penalties\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for logistic regression for new df\n",
    "\n",
    "model = LogisticRegression()\n",
    "results_df = pd.DataFrame(columns=['Data Type', 'Accuracy'])\n",
    "\n",
    "for x_col in x_cols_new:\n",
    "    accuracy = grid_search(df_new, x_cols_new[x_col], model, param_grid, y_col, 5)\n",
    "    results_df = results_df.append({'Data Type': x_col, 'Accuracy': accuracy}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_LR_new = results_df\n",
    "results_LR_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TU SPRAWDZAMY SKUTECZNOŚĆ NA CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu są zdefiniowane modele, ja je definiowałem trochę z buta, trochę z wcześniejszych wyników gridSearchu\n",
    "# Dobrze by to wyglądało jakbyś tu np. porównał modele dla konkretnych typów. Czyli np. tabela z mean_accuracy i std.dev\n",
    "# dla DecisionTreeClassifier kla każdego typu danych i np. różnych parametrów (np. criterion = 'gini' lub 'entropy')\n",
    "# I porównać normalny DecisionTreeClassifier z BaggingClassifier(DecisionTreeClassifier()) i tak dla każdej z grup modeli\n",
    "# mamy tu DecisionTree, SVC, LogisticRegreesion i RandomForest, możesz też spróbować policzyć KNN bo tego nie liczyłem a może wyniki będą spoko\n",
    "\n",
    "\n",
    "\n",
    "modelDT_1 = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=4,\n",
    "                       min_samples_split=10)\n",
    "modelDT_B1 = BaggingClassifier(DecisionTreeClassifier(), random_state = 42)\n",
    "\n",
    "model3 = SVC(kernel = 'rbf',C=20, gamma='scale')\n",
    "model4 = LogisticRegression(penalty = 'l2', fit_intercept = False, solver = 'lbfgs')\n",
    "\n",
    "model6 = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "\n",
    "models = [modelDT_1, model3, model4, modelDT_B1, model6] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validation_accuracy(models, df, x_cols, y_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=['Mean Validation Accuracy'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10,20],\n",
    "    'gamma': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, scoring='balanced_accuracy', cv=5)\n",
    "\n",
    "grid_search.fit(df.loc[:, boruta], df.loc[:,y_col])\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(test.loc[:, boruta])\n",
    "accuracy = balanced_accuracy_score(test.loc[:,y_col], y_pred)\n",
    "confusion = confusion_matrix(test.loc[:,y_col], y_pred)\n",
    "print(confusion)\n",
    "accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(DecisionTreeClassifier(), random_state = 42)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "n_scores = cross_val_score(model, df.loc[:, boruta], df.loc[:,y_col], scoring='balanced_accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Balanced Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df.loc[:, boruta], df.loc[:, y_col])\n",
    "predictions = model.predict(test.loc[:, boruta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc = balanced_accuracy_score(test.loc[:, y_col], predictions)\n",
    "confusion_mat = confusion_matrix(test.loc[:, y_col], predictions)\n",
    "print('Balanced Accuracy: %.3f' % balanced_acc)\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_series = pd.Series(predictions, name = 'Prediction')\n",
    "predictions_series.reset_index(drop=True, inplace=True)\n",
    "test.loc[:, y_col].reset_index(drop=True, inplace=True)\n",
    "results_df = pd.concat([predictions_series, test.loc[:, y_col]], axis=1)\n",
    "results_df[results_df['Prediction'] != results_df['account_status']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
